{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d0bd755",
   "metadata": {},
   "source": [
    "\n",
    "# K-Means Song Mood Clustering — EDA → Kesimpulan\n",
    "Notebook ini mengklaster **lagu** berdasarkan **fitur audio** (valence, energy, danceability, acousticness, instrumentalness, dll.) menggunakan **K-Means**.\n",
    "Struktur:\n",
    "1. Config & Load Data  \n",
    "2. EDA (missingness, histogram, korelasi)  \n",
    "3. Feature Engineering (log tempo, duration menit, encoding key/mode, loudness z)  \n",
    "4. Scaling & (opsional) PCA  \n",
    "5. Pemilihan k: Elbow + Silhouette + Calinski–Harabasz + Davies–Bouldin  \n",
    "6. K-Means/Minibatch final (auto-pick)  \n",
    "7. Profiling klaster (mean/std fitur, representative tracks)  \n",
    "8. Visualisasi: PCA 2D scatter, ukuran klaster, kurva metrik vs k  \n",
    "9. Validasi eksternal: Purity & NMI vs genre (jika tersedia)  \n",
    "10. Stability bootstrap (ARI)  \n",
    "11. Ablasi: RAW vs PCA  \n",
    "12. Kesimpulan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32481c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = Path(\"/mnt/data/your_spotify_audio_features.csv\")  # Ganti path\n",
    "\n",
    "COLS_CANDIDATES = {\n",
    "    \"track_id\":   [\"track_id\",\"id\",\"uri\",\"track_uri\"],\n",
    "    \"track_name\": [\"track_name\",\"name\",\"track\",\"song\",\"title\"],\n",
    "    \"artist\":     [\"artist\",\"artists\",\"artist_name\",\"artist_names\"],\n",
    "    \"genre\":      [\"genre\",\"playlist_genre\",\"tag\",\"primary_genre\"],\n",
    "    \"valence\":    [\"valence\"],\n",
    "    \"energy\":     [\"energy\"],\n",
    "    \"danceability\": [\"danceability\"],\n",
    "    \"acousticness\": [\"acousticness\"],\n",
    "    \"instrumentalness\": [\"instrumentalness\"],\n",
    "    \"liveness\":   [\"liveness\"],\n",
    "    \"speechiness\":[\"speechiness\"],\n",
    "    \"tempo\":      [\"tempo\",\"bpm\"],\n",
    "    \"loudness\":   [\"loudness\"],\n",
    "    \"duration_ms\":[\"duration_ms\",\"duration\"],\n",
    "    \"time_signature\": [\"time_signature\",\"timesig\"],\n",
    "    \"key\":        [\"key\"],\n",
    "    \"mode\":       [\"mode\"]\n",
    "}\n",
    "\n",
    "SAMPLE_EDA = 100000\n",
    "K_RANGE = list(range(6, 15))\n",
    "USE_PCA_FOR_CLUSTER = False\n",
    "N_PCA_COMPONENTS = 20\n",
    "MINIBATCH_THRESHOLD = 300_000\n",
    "MINIBATCH_BATCH_SIZE = 4096\n",
    "BOOTSTRAP_RUNS = 20\n",
    "BOOTSTRAP_FRAC = 0.8\n",
    "RANDOM_STATE = 42\n",
    "PRINT_ROWS = 10\n",
    "print(\"Config loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d833e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def autodetect_columns(df, candidates):\n",
    "    mapping = {}\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    for key, cand_list in candidates.items():\n",
    "        found = None\n",
    "        for cand in cand_list:\n",
    "            if cand.lower() in cols_lower:\n",
    "                found = cols_lower[cand.lower()]\n",
    "                break\n",
    "        mapping[key] = found\n",
    "    return mapping\n",
    "\n",
    "def read_data(path: Path, nrows=None):\n",
    "    return pd.read_csv(path, nrows=nrows)\n",
    "\n",
    "df_head = read_data(DATA_PATH, nrows=5000)\n",
    "colmap = autodetect_columns(df_head, COLS_CANDIDATES)\n",
    "print(\"Column mapping (auto):\", colmap)\n",
    "\n",
    "df = read_data(DATA_PATH, nrows=None)\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head(PRINT_ROWS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f863566",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EDA\n",
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "display(missing.head(30))\n",
    "\n",
    "num_keys = [\"valence\",\"energy\",\"danceability\",\"acousticness\",\"instrumentalness\",\n",
    "            \"liveness\",\"speechiness\",\"tempo\",\"loudness\",\"duration_ms\",\"time_signature\",\"key\",\"mode\"]\n",
    "num_cols = [colmap[k] for k in num_keys if colmap.get(k) is not None]\n",
    "num_cols = [c for c in num_cols if c in df.columns]\n",
    "\n",
    "if len(df) > SAMPLE_EDA:\n",
    "    df_eda = df.sample(SAMPLE_EDA, random_state=RANDOM_STATE)\n",
    "else:\n",
    "    df_eda = df.copy()\n",
    "\n",
    "print(\"Numeric columns used for EDA:\", num_cols)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for c in num_cols[:8]:\n",
    "    plt.figure()\n",
    "    df_eda[c].dropna().astype(float).hist(bins=50)\n",
    "    plt.title(f\"Histogram: {c}\")\n",
    "    plt.xlabel(c); plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if len(num_cols) >= 2:\n",
    "    corr = df_eda[num_cols].corr(numeric_only=True)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    im = plt.imshow(corr.values)\n",
    "    plt.xticks(range(len(num_cols)), num_cols, rotation=90)\n",
    "    plt.yticks(range(len(num_cols)), num_cols)\n",
    "    plt.title(\"Correlation (Pearson)\")\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature Engineering\n",
    "work = df.copy()\n",
    "\n",
    "if colmap.get(\"duration_ms\") and colmap[\"duration_ms\"] in work.columns:\n",
    "    work[\"duration_min\"] = work[colmap[\"duration_ms\"]] / 60000.0\n",
    "else:\n",
    "    work[\"duration_min\"] = np.nan\n",
    "\n",
    "if colmap.get(\"tempo\") and colmap[\"tempo\"] in work.columns:\n",
    "    work[\"log_tempo\"] = np.log1p(work[colmap[\"tempo\"]])\n",
    "else:\n",
    "    work[\"log_tempo\"] = np.nan\n",
    "\n",
    "if colmap.get(\"key\") and colmap[\"key\"] in work.columns:\n",
    "    key_raw = work[colmap[\"key\"]].astype(float)\n",
    "    work[\"key_sin\"] = np.sin(2*np.pi*key_raw/12.0)\n",
    "    work[\"key_cos\"] = np.cos(2*np.pi*key_raw/12.0)\n",
    "else:\n",
    "    work[\"key_sin\"] = np.nan\n",
    "    work[\"key_cos\"] = np.nan\n",
    "\n",
    "if colmap.get(\"mode\") and colmap[\"mode\"] in work.columns:\n",
    "    work[\"mode_bin\"] = work[colmap[\"mode\"]].astype(float)\n",
    "else:\n",
    "    work[\"mode_bin\"] = np.nan\n",
    "\n",
    "if colmap.get(\"loudness\") and colmap[\"loudness\"] in work.columns:\n",
    "    loud = work[colmap[\"loudness\"]].astype(float)\n",
    "    work[\"loudness_z\"] = (loud - loud.mean()) / (loud.std(ddof=0) + 1e-9)\n",
    "else:\n",
    "    work[\"loudness_z\"] = np.nan\n",
    "\n",
    "feature_list = []\n",
    "for k in [\"valence\",\"energy\",\"danceability\",\"acousticness\",\"instrumentalness\",\"liveness\",\"speechiness\"]:\n",
    "    if colmap.get(k):\n",
    "        feature_list.append(colmap[k])\n",
    "\n",
    "for extra in [\"log_tempo\",\"loudness_z\",\"duration_min\",\"key_sin\",\"key_cos\",\"mode_bin\",\"time_signature\"]:\n",
    "    if (extra in work.columns) or (colmap.get(extra) and colmap[extra] in work.columns):\n",
    "        feature_list.append(extra if extra in work.columns else colmap[extra])\n",
    "\n",
    "X = work[feature_list].copy()\n",
    "pre_rows = len(X)\n",
    "X = X.dropna()\n",
    "print(f\"Dropped rows due to NaN in features: {pre_rows - len(X)}\")\n",
    "\n",
    "id_col   = colmap.get(\"track_id\")\n",
    "name_col = colmap.get(\"track_name\")\n",
    "artist_col = colmap.get(\"artist\")\n",
    "meta_cols = [c for c in [id_col, name_col, artist_col, colmap.get(\"genre\")] if c]\n",
    "meta = work.loc[X.index, meta_cols] if meta_cols else None\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Feature columns:\", list(X.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scaling & PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X.values)\n",
    "\n",
    "USE_PCA_FOR_CLUSTER = False\n",
    "N_PCA_COMPONENTS = 20\n",
    "if USE_PCA_FOR_CLUSTER:\n",
    "    pca = PCA(n_components=N_PCA_COMPONENTS, random_state=42)\n",
    "    X_model = pca.fit_transform(X_scaled)\n",
    "    print(\"PCA explained variance ratio (cum):\", pca.explained_variance_ratio_.sum())\n",
    "else:\n",
    "    X_model = X_scaled\n",
    "\n",
    "print(\"Model space:\", X_model.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# K selection\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "MINIBATCH_THRESHOLD = 300_000\n",
    "MINIBATCH_BATCH_SIZE = 4096\n",
    "use_minibatch = (len(X_model) > MINIBATCH_THRESHOLD)\n",
    "print(\"Using MiniBatchKMeans:\", use_minibatch)\n",
    "\n",
    "rows = []; inertias = []\n",
    "for k in range(6, 15):\n",
    "    if use_minibatch:\n",
    "        km = MiniBatchKMeans(n_clusters=k, batch_size=MINIBATCH_BATCH_SIZE, n_init=10, random_state=42)\n",
    "    else:\n",
    "        km = KMeans(n_clusters=k, n_init=15, random_state=42)\n",
    "    lab = km.fit_predict(X_model)\n",
    "    inertias.append(km.inertia_)\n",
    "    try: sil = silhouette_score(X_model, lab)\n",
    "    except: sil = np.nan\n",
    "    try: ch = calinski_harabasz_score(X_model, lab)\n",
    "    except: ch = np.nan\n",
    "    try: db = davies_bouldin_score(X_model, lab)\n",
    "    except: db = np.nan\n",
    "    rows.append((k, sil, ch, db))\n",
    "\n",
    "metrics_df = pd.DataFrame(rows, columns=['k','silhouette','calinski_harabasz','davies_bouldin'])\n",
    "display(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plots for K selection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(metrics_df['k'], inertias, marker='o')\n",
    "plt.title(\"Elbow Plot (Inertia vs k)\")\n",
    "plt.xlabel(\"k\"); plt.ylabel(\"Inertia\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(metrics_df['k'], metrics_df['silhouette'], marker='o')\n",
    "plt.title(\"Silhouette vs k\")\n",
    "plt.xlabel(\"k\"); plt.ylabel(\"Silhouette\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(metrics_df['k'], metrics_df['calinski_harabasz'], marker='o')\n",
    "plt.title(\"Calinski–Harabasz vs k\")\n",
    "plt.xlabel(\"k\"); plt.ylabel(\"CH score\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(metrics_df['k'], metrics_df['davies_bouldin'], marker='o')\n",
    "plt.title(\"Davies–Bouldin vs k (lower better)\")\n",
    "plt.xlabel(\"k\"); plt.ylabel(\"DB index\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdaee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose best k (combined ranking)\n",
    "dfc = metrics_df.copy()\n",
    "dfc['rank_sil'] = (-dfc['silhouette']).rank(method='min')\n",
    "dfc['rank_ch']  = (-dfc['calinski_harabasz']).rank(method='min')\n",
    "dfc['rank_db']  = (dfc['davies_bouldin']).rank(method='min')\n",
    "dfc['rank_sum'] = dfc[['rank_sil','rank_ch','rank_db']].sum(axis=1)\n",
    "\n",
    "best_row = dfc.sort_values(['rank_sum','k']).iloc[0]\n",
    "best_k = int(best_row['k'])\n",
    "print(\"Best k:\", best_k)\n",
    "display(dfc.sort_values('rank_sum').head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20919327",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Final KMeans fit\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "if use_minibatch:\n",
    "    final_km = MiniBatchKMeans(n_clusters=best_k, batch_size=MINIBATCH_BATCH_SIZE, n_init=20, random_state=42)\n",
    "else:\n",
    "    final_km = KMeans(n_clusters=best_k, n_init=25, random_state=42)\n",
    "\n",
    "labels = final_km.fit_predict(X_model)\n",
    "\n",
    "# Back to original scale centroids\n",
    "centroids_model = final_km.cluster_centers_\n",
    "if USE_PCA_FOR_CLUSTER:\n",
    "    X_scaled_cent = pca.inverse_transform(centroids_model)\n",
    "else:\n",
    "    X_scaled_cent = centroids_model\n",
    "\n",
    "centroids_orig = pd.DataFrame(\n",
    "    data = (X_scaled_cent * scaler.scale_) + scaler.mean_,\n",
    "    columns = list(X.columns)\n",
    ")\n",
    "centroids_orig.insert(0, 'Cluster', range(best_k))\n",
    "\n",
    "assignments = pd.DataFrame({'Cluster': labels}, index=X.index).reset_index()\n",
    "if meta is not None:\n",
    "    assign_out = pd.concat([meta.reset_index(drop=True), assignments], axis=1)\n",
    "else:\n",
    "    assign_out = assignments.copy()\n",
    "\n",
    "display(assign_out.head(20))\n",
    "display(centroids_orig.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f5502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Profiling & representatives\n",
    "feat_cols = list(X.columns)\n",
    "df_lab = pd.DataFrame({'Cluster': labels}, index=X.index)\n",
    "X_prof = pd.concat([X.reset_index(drop=True), df_lab.reset_index(drop=True)], axis=1)\n",
    "\n",
    "profile_mean = X_prof.groupby('Cluster')[feat_cols].mean()\n",
    "profile_std  = X_prof.groupby('Cluster')[feat_cols].std(ddof=0)\n",
    "display(profile_mean.head(20)); display(profile_std.head(20))\n",
    "\n",
    "# Representatives (closest to centroid in model space)\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def representative_indices_per_cluster(Xm, labels, centers, topn=10):\n",
    "    reps = {}\n",
    "    for c in range(centers.shape[0]):\n",
    "        idx = np.where(labels == c)[0]\n",
    "        if len(idx) == 0: \n",
    "            reps[c] = []\n",
    "            continue\n",
    "        Xm_c = Xm[idx]\n",
    "        d = norm(Xm_c - centers[c], axis=1)\n",
    "        order = np.argsort(d)[:topn]\n",
    "        reps[c] = idx[order]\n",
    "    return reps\n",
    "\n",
    "reps = representative_indices_per_cluster(X_model, labels, final_km.cluster_centers_, topn=10)\n",
    "\n",
    "if meta is not None and (colmap.get('track_name') in meta.columns if meta is not None else False):\n",
    "    rows = []\n",
    "    for c, idxs in reps.items():\n",
    "        for j in idxs:\n",
    "            tr = meta.iloc[j] if meta is not None else {}\n",
    "            rows.append((c, tr.get(colmap.get('track_name'), None), tr.get(colmap.get('artist'), None)))\n",
    "    rep_df = pd.DataFrame(rows, columns=['Cluster','Track','Artist'])\n",
    "    display(rep_df.head(50))\n",
    "else:\n",
    "    print({k: list(v) for k,v in reps.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02cc3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualizations\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pca2 = PCA(n_components=2, random_state=42)\n",
    "X_p2 = pca2.fit_transform(X_model)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "for c in range(best_k):\n",
    "    m = (labels == c)\n",
    "    plt.scatter(X_p2[m,0], X_p2[m,1], s=5, label=f'Cluster {c}')\n",
    "plt.title(f\"K-Means Clusters (PCA 2D) — k={best_k}\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.legend(markerscale=3, loc='best')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "size_df = pd.DataFrame({'Cluster': unique, 'Count': counts}).sort_values('Count', ascending=False)\n",
    "display(size_df)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(size_df['Cluster'].astype(str), size_df['Count'].values)\n",
    "plt.title(\"Cluster Size Distribution\")\n",
    "plt.xlabel(\"Cluster\"); plt.ylabel(\"Count\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0986bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# External validation vs genre\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    labels = np.unique(y_pred)\n",
    "    total = len(y_true)\n",
    "    correct = 0\n",
    "    for l in labels:\n",
    "        idx = np.where(y_pred == l)[0]\n",
    "        if len(idx) == 0: \n",
    "            continue\n",
    "        vals, cnts = np.unique(y_true[idx], return_counts=True)\n",
    "        correct += cnts.max()\n",
    "    return correct / total if total > 0 else np.nan\n",
    "\n",
    "if colmap.get(\"genre\") and (colmap[\"genre\"] in df.columns):\n",
    "    g = work.loc[X.index, colmap[\"genre\"]].astype(str).values\n",
    "    purity = purity_score(g, labels)\n",
    "    nmi = normalized_mutual_info_score(g, labels)\n",
    "    print(\"Purity vs genre:\", purity)\n",
    "    print(\"NMI vs genre:\", nmi)\n",
    "    tmp = pd.DataFrame({'Cluster': labels, 'genre': g})\n",
    "    topg = tmp.groupby('Cluster')['genre'].agg(lambda x: x.value_counts().head(3).to_dict())\n",
    "    display(topg)\n",
    "else:\n",
    "    print(\"Kolom genre tidak tersedia — lewati validasi eksternal ini.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526a698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stability bootstrap (ARI)\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "BASE_LABELS = labels\n",
    "B = 20\n",
    "aris = []\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "for b in range(B):\n",
    "    idx = rng.choice(len(X_model), size=int(0.8*len(X_model)), replace=True)\n",
    "    Xm = X_model[idx]\n",
    "    if use_minibatch:\n",
    "        km_b = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=10, random_state=42+b).fit(Xm)\n",
    "    else:\n",
    "        km_b = KMeans(n_clusters=best_k, n_init=10, random_state=42+b).fit(Xm)\n",
    "    pred_full = km_b.predict(X_model)\n",
    "    ari = adjusted_rand_score(BASE_LABELS, pred_full)\n",
    "    aris.append(ari)\n",
    "\n",
    "print(f\"Bootstrap ARI (B={B}): mean={np.mean(aris):.3f}, std={np.std(aris):.3f}, min={np.min(aris):.3f}, max={np.max(aris):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d7529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ablation: RAW vs PCA10\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "pca_ab = PCA(n_components=min(10, X_scaled.shape[1]), random_state=42)\n",
    "X_pca_ab = pca_ab.fit_transform(X_scaled)\n",
    "\n",
    "def fit_scores(Xm, k, minibatch=False):\n",
    "    if minibatch:\n",
    "        km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=10, random_state=42)\n",
    "    else:\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "    lab = km.fit_predict(Xm)\n",
    "    try: sil = silhouette_score(Xm, lab)\n",
    "    except: sil = np.nan\n",
    "    try: ch = calinski_harabasz_score(Xm, lab)\n",
    "    except: ch = np.nan\n",
    "    try: db = davies_bouldin_score(Xm, lab)\n",
    "    except: db = np.nan\n",
    "    return sil, ch, db\n",
    "\n",
    "sil_raw, ch_raw, db_raw = fit_scores(X_model, best_k, minibatch=use_minibatch)\n",
    "sil_pca, ch_pca, db_pca = fit_scores(X_pca_ab, best_k, minibatch=use_minibatch)\n",
    "\n",
    "print(\"RAW  -> Sil:\", sil_raw, \" CH:\", ch_raw, \" DB:\", db_raw)\n",
    "print(\"PCA10-> Sil:\", sil_pca, \" CH:\", ch_pca, \" DB:\", db_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b33cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Export outputs\n",
    "out_dir = Path(\"/mnt/data\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "labels_path = out_dir / \"kmeans_labels.csv\"\n",
    "centroids_path = out_dir / \"kmeans_centroids_original_scale.csv\"\n",
    "profiles_mean_path = out_dir / \"kmeans_cluster_profile_mean.csv\"\n",
    "profiles_std_path  = out_dir / \"kmeans_cluster_profile_std.csv\"\n",
    "metrics_path = out_dir / \"kmeans_metrics_vs_k.csv\"\n",
    "\n",
    "assign_out.to_csv(labels_path, index=False)\n",
    "centroids_orig.to_csv(centroids_path, index=False)\n",
    "profile_mean.to_csv(profiles_mean_path)\n",
    "profile_std.to_csv(profiles_std_path)\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(labels_path)\n",
    "print(centroids_path)\n",
    "print(profiles_mean_path)\n",
    "print(profiles_std_path)\n",
    "print(metrics_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87283bd",
   "metadata": {},
   "source": [
    "\n",
    "## Kesimpulan (Template)\n",
    "- **Tujuan**: Mengidentifikasi arketipe mood/typology lagu menggunakan **K-Means** pada fitur audio berskala besar.\n",
    "- **Pemilihan k**: Gabungan **Silhouette/CH/DB** → k = ___ (kompromi stabil & interpretabel).\n",
    "- **Hasil**: ___ klaster; karakter ringkas per klaster; contoh lagu/artis representatif.\n",
    "- **Validasi**: Purity = ___; NMI = ___ terhadap **genre** (jika tersedia).\n",
    "- **Stabilitas**: Bootstrap ARI = mean ___ ± ___ (___).\n",
    "- **Ablasi**: Perbandingan RAW vs PCA10 menunjukkan ___; konfigurasi final menggunakan ___.\n",
    "- **Keterbatasan**: Sensitivitas terhadap skala & korelasi fitur; asumsi bentuk cluster sferis.\n",
    "- **Kerja lanjut**: Micro-cluster per genre, trend per dekade, bandingkan dengan HDBSCAN atau spherical K-Means.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
