{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b89f2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact NN (queries=1000) done in 374.492 s\n",
      "Annoy build: 9.029 s, query all: 37.124 s\n",
      "HNSW build: 14.649 s, query all: 12.911 s\n",
      "FAISS build: 0.087 s, query all: 71.344 s\n",
      "\n",
      "Summary (build time | query time for sampled points | recall@k)\n",
      "Exact:  - | 374.492 s (queries only) | recall=1.00\n",
      "Annoy:  9.029 s | 37.124 s | recall@10=0.9945\n",
      "HNSW:   14.649 s | 12.911 s | recall@10=0.9936\n",
      "FAISS:  0.087 s | 71.344 s | recall@10=0.9982\n",
      "\n",
      "Top-5 neighbors for first sampled query (dataset index = 287796)\n",
      "Exact NN: [     0 394553 764272 837727 749223]\n",
      "Annoy:    [0, 394553, 764272, 837727, 749223]\n",
      "HNSW:     [     0 394553 764272 837727 749223]\n",
      "FAISS:    [     0 394553 764272 837727 749223]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import faiss\n",
    "from annoy import AnnoyIndex\n",
    "import hnswlib\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use all available CPU cores where possible\n",
    "n_cores = os.cpu_count() or 1\n",
    "os.environ.setdefault('OMP_NUM_THREADS', str(n_cores))\n",
    "os.environ.setdefault('OPENBLAS_NUM_THREADS', str(n_cores))\n",
    "os.environ.setdefault('MKL_NUM_THREADS', str(n_cores))\n",
    "# Tell faiss to use multiple threads (if built with OpenMP)\n",
    "try:\n",
    "    faiss.omp_set_num_threads(n_cores)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------------\n",
    "# Load dataset (drop NaNs in chosen features)\n",
    "# -------------------------------\n",
    "df = pd.read_csv(f'data/songs_with_attributes_and_lyrics.csv')  # ganti path sesuai lokasi file\n",
    "features = ['danceability', 'energy', 'loudness', 'speechiness',\n",
    "            'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n",
    "df = df[features].dropna().reset_index(drop=True)\n",
    "X = df.values\n",
    "\n",
    "# Standardize and cast to float32 (required by faiss/hnswlib)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X).astype(np.float32)\n",
    "\n",
    "n = X_scaled.shape[0]\n",
    "k = 10  # jumlah nearest neighbors\n",
    "# To keep this runnable on limited RAM, sample up to 1000 query points\n",
    "n_queries = min(1000, n)\n",
    "rng = np.random.default_rng(42)\n",
    "query_idx = rng.choice(n, size=n_queries, replace=False)\n",
    "# Xq = X_scaled[query_idx]\n",
    "Xq = X_scaled\n",
    "\n",
    "# -------------------------------\n",
    "# Exact Nearest Neighbor (brute-force) - only for the sampled queries\n",
    "# -------------------------------\n",
    "t0 = time.time()\n",
    "nn = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean', n_jobs=-1)\n",
    "nn.fit(X_scaled)\n",
    "dist_exact, idx_exact = nn.kneighbors(Xq)\n",
    "time_exact = time.time() - t0\n",
    "print(f\"Exact NN (queries={n_queries}) done in {time_exact:.3f} s\")\n",
    "\n",
    "# -------------------------------\n",
    "# Annoy (build + query on sampled points)\n",
    "# -------------------------------\n",
    "t0 = time.time()\n",
    "fdim = X_scaled.shape[1]\n",
    "index_annoy = AnnoyIndex(fdim, 'euclidean')\n",
    "for i, v in enumerate(X_scaled):\n",
    "    index_annoy.add_item(i, v.tolist())\n",
    "n_trees = 50\n",
    "index_annoy.build(n_trees)\n",
    "t_build_annoy = time.time() - t0\n",
    "\n",
    "tq = time.time()\n",
    "# Annoy: parallelize queries using joblib (threading) to utilize multiple cores\n",
    "def _query_annoy(v):\n",
    "    return index_annoy.get_nns_by_vector(v.tolist(), k)\n",
    "idx_annoy = Parallel(n_jobs=n_cores, prefer='threads')(delayed(_query_annoy)(v) for v in Xq)\n",
    "time_query_annoy = time.time() - tq\n",
    "print(f\"Annoy build: {t_build_annoy:.3f} s, query all: {time_query_annoy:.3f} s\")\n",
    "\n",
    "# -------------------------------\n",
    "# HNSW (hnswlib)\n",
    "# -------------------------------\n",
    "t0 = time.time()\n",
    "p = hnswlib.Index(space='l2', dim=fdim)\n",
    "p.init_index(max_elements=n, ef_construction=200, M=16)\n",
    "p.add_items(X_scaled)\n",
    "p.set_ef(200)\n",
    "t_build_hnsw = time.time() - t0\n",
    "\n",
    "tq = time.time()\n",
    "# hnswlib supports num_threads in knn_query\n",
    "idx_hnsw, dist_hnsw = p.knn_query(Xq, k=k, num_threads=n_cores)\n",
    "time_query_hnsw = time.time() - tq\n",
    "print(f\"HNSW build: {t_build_hnsw:.3f} s, query all: {time_query_hnsw:.3f} s\")\n",
    "\n",
    "# -------------------------------\n",
    "# FAISS IVF (train on full set, query sampled points)\n",
    "# -------------------------------\n",
    "t0 = time.time()\n",
    "quantizer = faiss.IndexFlatL2(fdim)\n",
    "nlist = 100\n",
    "index_faiss = faiss.IndexIVFFlat(quantizer, fdim, nlist, faiss.METRIC_L2)\n",
    "# FAISS requires float32 and contiguous arrays\n",
    "index_faiss.train(np.ascontiguousarray(X_scaled))\n",
    "index_faiss.add(np.ascontiguousarray(X_scaled))\n",
    "index_faiss.nprobe = 10\n",
    "t_build_faiss = time.time() - t0\n",
    "\n",
    "tq = time.time()\n",
    "# FAISS can use multiple threads via set_num_threads if available\n",
    "try:\n",
    "    faiss.omp_set_num_threads(n_cores)\n",
    "except Exception:\n",
    "    pass\n",
    "D_faiss, idx_faiss = index_faiss.search(np.ascontiguousarray(Xq), k)\n",
    "time_query_faiss = time.time() - tq\n",
    "print(f\"FAISS build: {t_build_faiss:.3f} s, query all: {time_query_faiss:.3f} s\")\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluate recall@k for each ANN vs exact\n",
    "# -------------------------------\n",
    "def recall_at_k(true_idx, pred_idx, k):\n",
    "    # true_idx: (n_queries, k), pred_idx: iterable of length n_queries with lists/arrays\n",
    "    total = 0.0\n",
    "    n = len(true_idx)\n",
    "    for t, p in zip(true_idx, pred_idx):\n",
    "        pset = set(p.tolist() if hasattr(p, 'tolist') else p)\n",
    "        total += len(pset.intersection(set(t[:k]))) / float(k)\n",
    "    return total / n\n",
    "\n",
    "rec_annoy = recall_at_k(idx_exact, idx_annoy, k)\n",
    "rec_hnsw = recall_at_k(idx_exact, idx_hnsw, k)\n",
    "rec_faiss = recall_at_k(idx_exact, idx_faiss, k)\n",
    "\n",
    "print('\\nSummary (build time | query time for sampled points | recall@k)')\n",
    "print(f\"Exact:  - | {time_exact:.3f} s (queries only) | recall=1.00\")\n",
    "print(f\"Annoy:  {t_build_annoy:.3f} s | {time_query_annoy:.3f} s | recall@{k}={rec_annoy:.4f}\")\n",
    "print(f\"HNSW:   {t_build_hnsw:.3f} s | {time_query_hnsw:.3f} s | recall@{k}={rec_hnsw:.4f}\")\n",
    "print(f\"FAISS:  {t_build_faiss:.3f} s | {time_query_faiss:.3f} s | recall@{k}={rec_faiss:.4f}\")\n",
    "\n",
    "# show top-5 neighbors for the first sampled query (original dataset index)\n",
    "qid = query_idx[0]\n",
    "print(\"\\nTop-5 neighbors for first sampled query (dataset index = {})\".format(int(qid)))\n",
    "print(f\"Exact NN: {idx_exact[0][:5]}\")\n",
    "print(f\"Annoy:    {idx_annoy[0][:5]}\")\n",
    "print(f\"HNSW:     {idx_hnsw[0][:5]}\")\n",
    "print(f\"FAISS:    {idx_faiss[0][:5]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
